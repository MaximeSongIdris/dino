{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8afb7348-7c7d-4a52-bdf1-b2b00c1ff82f",
   "metadata": {},
   "source": [
    "## **CHECK SIMPLE AUGMENTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf4d06-ae26-4b71-9601-19f248c34081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# arg definition\n",
    "data_path = \"/gpfsdswork/dataset/imagenet/train\"\n",
    "global_crops_scale = (0.8, 1.)\n",
    "\n",
    "# ============ preparing data ... ============\n",
    "transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "dataset = datasets.ImageFolder(data_path, transform=transform)\n",
    "original_dataset = datasets.ImageFolder(data_path, transform=None)\n",
    "print(f\"Data loaded: there are {len(dataset)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8d753-2055-44c4-a51e-c3ee2d6e76cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "n = randint(0,10000)\n",
    "img_resized_crop = dataset[n]\n",
    "img = original_dataset[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e582c54-da8b-4afc-8adc-61050ce0775d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2013a1-4b85-4e33-8c9d-574df70e2589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms.ToPILImage()(img_resized_crop[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210a47a-ff93-4369-8e3f-e598b1dd13f1",
   "metadata": {},
   "source": [
    "## **CHECK DINO DATA AUGMENTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8df5d-d1d9-4faf-9128-b0a1f98e46bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import utils\n",
    "\n",
    "class DataAugmentationDINO(object):\n",
    "    def __init__(self, global_crops_scale, local_crops_scale, local_crops_number):\n",
    "        flip_and_color_jitter = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "        ])\n",
    "        normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "        # first global crop\n",
    "        self.global_transfo1 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(1.0),\n",
    "            normalize,\n",
    "        ])\n",
    "        # second global crop\n",
    "        self.global_transfo2 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(0.1),\n",
    "            utils.Solarization(0.2),\n",
    "            normalize,\n",
    "        ])\n",
    "        # transformation for the local small crops\n",
    "        self.local_crops_number = local_crops_number\n",
    "        self.local_transfo = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, scale=local_crops_scale, interpolation=Image.BICUBIC),\n",
    "            flip_and_color_jitter,\n",
    "            utils.GaussianBlur(p=0.5),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        crops = []\n",
    "        crops.append(self.global_transfo1(image))\n",
    "        crops.append(self.global_transfo2(image))\n",
    "        for _ in range(self.local_crops_number):\n",
    "            crops.append(self.local_transfo(image))\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d5ce15-a0ff-4fd2-9027-70cc18dd0b14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_dataset = datasets.ImageFolder('/gpfswork/rech/uli/ssos027/dino_experience/data/ImageNet/train', transform=None)\n",
    "data = original_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb9499-950d-4430-8b1c-2af28f72315a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transfo = DataAugmentationDINO(\n",
    "        (0.4, 1.0),\n",
    "        (0.05, 0.4),\n",
    "        6,\n",
    "    )\n",
    "transforms.ToPILImage()(transfo(data[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067872f-d17d-48ef-a0d2-224374a4342a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962aa1eb-e7f5-4f84-ab43-ddae785c85a0",
   "metadata": {},
   "source": [
    "## CHECK COCO DATASET DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0638e6-0426-479d-879f-663ace5234e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "\n",
    "\n",
    "# TODO : maybe reduce the number of classes for training \n",
    "# Check how the training is performed for object-centric representation on existing works\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        data_dir,\n",
    "        transform,\n",
    "    ):\n",
    "        super(COCODataset, self).__init__()\n",
    "        if dataset == 'COCO':\n",
    "            ann_file = data_dir + '/annotations/instances_train2017.json'\n",
    "            self.coco = COCO(ann_file)\n",
    "            self.ids = self.coco.getImgIds() # list of image id\n",
    "            self.cat_ids = self.coco.getCatIds() # list of cat id\n",
    "            self.root = data_dir + '/train2017/'\n",
    "            self.target_transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(224, interpolation=TF.InterpolationMode.NEAREST),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            self.num_cat = len(self.cat_ids) # TODO only use no crowd annotations ???\n",
    "        elif dataset == 'COCOplus':\n",
    "            self.fpaths = glob.glob(data_dir + '/train2017/*.jpg') + glob.glob(data_dir + '/unlabeled2017/*.jpg')\n",
    "            self.fpaths = np.array(self.fnames) # to avoid memory leak\n",
    "        elif dataset == 'COCOval':\n",
    "            self.fpaths = glob.glob(data_dir + '/val2017/*.jpg')\n",
    "            self.fpaths = np.array(self.fnames) # to avoid memory leak\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataset == 'COCO':\n",
    "            return len(self.ids)\n",
    "        return len(self.fpaths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.dataset == 'COCO': \n",
    "            img_id = self.ids[idx]\n",
    "            \n",
    "            # Load image\n",
    "            # type : PIL.Image.Image\n",
    "            # size : (W, H)\n",
    "            fname = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "            image = Image.open(os.path.join(self.root, fname)).convert('RGB')\n",
    "\n",
    "            # Get all the annotations linked to our image\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            \n",
    "            # for each pixel in the image, attribute its category id\n",
    "            # category id began from 1 to 99 with a total of 80 categories\n",
    "            # size : (H,W)\n",
    "            anns_img = np.zeros((image.size[1],image.size[0]), dtype=np.uint8)\n",
    "            for ann in anns:\n",
    "                anns_img = np.maximum(anns_img, self.coco.annToMask(ann)*ann['category_id'])\n",
    "            \n",
    "            # (H,W) != (224,224)\n",
    "            one_hot_mask = torch.zeros(224, 224, self.num_cat)\n",
    "            for i, class_id in enumerate(self.cat_ids):\n",
    "                # type : np.ndarray\n",
    "                # len : H*W\n",
    "                mask = (anns_img == class_id).astype(np.uint8)\n",
    "                # transform our np.ndarray to torch.tensor \n",
    "                # and resize from HxW to 224x224\n",
    "                one_hot_mask[..., i] = self.target_transform(mask)\n",
    "             \n",
    "            # transform image for training\n",
    "            transfo_img = self.transform(image)\n",
    "            \n",
    "            # transform our np.ndarray to torch.tensor \n",
    "            # and resize from HxW to 224x224\n",
    "            anns_img = self.target_transform(anns_img)\n",
    "            \n",
    "            # type : tuple[torch.tensor, tuple[torch.tensor, torch.tensor]]\n",
    "            return transfo_img, (anns_img, one_hot_mask)\n",
    "        \n",
    "        fpath = self.fnames[idx]\n",
    "        image = Image.open(fpath).convert('RGB')\n",
    "        # type : tuple[torch.tensor, None]\n",
    "        return self.transform(image), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8515d-c26d-495b-89ed-be074bbb47aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = 'COCO'\n",
    "data_dir = '/gpfswork/rech/uli/ssos027/dino_experience/data/COCO'\n",
    "transform = DataAugmentationDINO(\n",
    "        (0.7, 1.0),\n",
    "        (0.05, 0.4),\n",
    "        6,\n",
    "    )\n",
    "\n",
    "coco = COCODataset(dataset, data_dir, transform)\n",
    "img, transfo_img, _ = coco[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05bbf7-f7f4-4488-a0d3-1adf0c7e01ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = DataAugmentationDINO(\n",
    "        (0.7, 1.0),\n",
    "        (0.3, 0.7),\n",
    "        6,\n",
    "    )\n",
    "coco.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56693ee-595f-440e-89bd-b5d6c2c99bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transfo_img = coco.transform(img)\n",
    "transforms.ToPILImage()(transfo_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c73a1be-9575-4776-b990-eafbb2324ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms.ToPILImage()(transfo_img[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046dee0-99f5-46ce-a969-681b41828207",
   "metadata": {},
   "source": [
    "## **CHECK COCO API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de200e-6241-44c7-817f-90643dc04d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fefeaa-46d8-4553-b496-464cbed6649d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root = '/gpfswork/rech/uli/ssos027/dino_experience/data/COCO'\n",
    "img_root = root + '/train2017/'\n",
    "ann_file = root + '/annotations/instances_train2017.json'\n",
    "coco = COCO(ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe718c-fb09-4e0a-ab8c-2836bfcf9bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_id = coco.getImgIds()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea02196-78f7-4dc1-9331-ad2395865d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fname = coco.loadImgs(img_id)[0]['file_name']\n",
    "image = Image.open(os.path.join(img_root, fname)).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb648a6-8578-47ae-ac9f-d8d9e2779e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "anns = coco.loadAnns(ann_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe80cc-c6b2-44ed-8f41-eb8fbe28512c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anns_img = np.zeros((image.size[1],image.size[0]), dtype=np.uint8)\n",
    "for ann in anns:\n",
    "    anns_img = np.maximum(anns_img, coco.annToMask(ann)*ann['category_id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0.0_py3.10.9",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.0.0_py3.10.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
